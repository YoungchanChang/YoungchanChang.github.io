---
layout: post
title: 모두를 위한 딥러닝 08 - multi-variable linear regression 
category: ML
tags: [ML]
comments: ML
---

# Linear Regression을 구하기 위한 요건

### Hypothesis

- H(x) + b

- 데이터를 갖고 무엇을 예측하고 싶은지

### Cost function

- 예측을 잘 했는지 평가하는 방법 **(예측값 - 실제값)**

- squre제곱을 한다. `cost = (1/m)*∑(H(xi) - yi)²`

### Gradient descent algorithm

- cost를 최적화하는 알고리즘

- w에 해당하는 값을 경사면을 따라서 내려가게 한다.


# 여러개의 변수 예측하기

### 변수가 여러개일 때 값을 예측하기

- 1개일 때는 H(x) = W * x + b가 된다.

- 3개일 때는 H(x1, x2, x3) = w1 * x1 + w2 * x2 + w3 * x3 + b

- cost Function은 예측값에 대한 차이 그대로 쓰면 된다.

- `cost(W, b) = (1/m)*∑(H(x1, x2, x3) - yi)²` 다만 가설만 바뀐다.

### Matrix

- 다양한 값을 처리하기 위해서 Matrix를 이용할 수 있다. Matrix 곱셈을 이용할 수 있다.

- Matrix를 이용해서 가설을 그대로 표현할 수 있다.
```
[1  2  3] x [7 8] = [ ]
```

- H(X) = X * W가 될 수 있다. X와 W는 매트릭스 곱이 된다.

- 데이터 하나하나를 인스턴스라고 부른다. 각각을 연산을 할 때, 여러번 반복해서 계산을 할 수 있다.

- Matrix의 장점 : 인스턴스의 수대로 Matrix를 쓸 수 있다. W는 이전과 똑같이 쓸 수 있다.

- Matrix곱을 하면 된다.

- 인스턴스가 많을 때 전체를 긴 매트릭스에 넣으면 된다. w를 곱해버리면 한 번에 나오게 된다.

- **W의 값을 구해야 되는데** 여기서 X의 matrix와 H(x)의 matrix의 값을 하나씩 가져오면 된다.

- [n, 3] * [?, ?] = [n, 2]에서 3,2르 w를 사용할 수 있다.

- H(X) = X * W형태로 많이 사용하게 된다.


### Matrix를 이용해서 코드를 작성하기

- 아래 코드는 기존의 가설이 확장된 코드이다.

```python
import numpy as np
import matplotlib.pyplot as plt
import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()


x1_data = [73., 93., 89., 96., 73.]
x2_data = [80., 88., 91., 98., 66.]
x3_data = [75., 93., 90., 100., 70.]
y_data = [152., 185., 180., 196., 142.]

x1 = tf.placeholder(tf.float32)
x2 = tf.placeholder(tf.float32)
x3 = tf.placeholder(tf.float32)

Y = tf.placeholder(tf.float32)

w1 = tf.Variable(tf.random_normal([1]), name='weight1')
w2 = tf.Variable(tf.random_normal([1]), name='weight2')
w3 = tf.Variable(tf.random_normal([1]), name='weight3')
b = tf.Variable(tf.random_normal([1]), name='bias')

hypothesis = x1 * w1 + x2 * w2 + x3 * w3 + b

cost = tf.reduce_mean(tf.square(hypothesis - Y))

optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-5)
train = optimizer.minimize(cost)

sess = tf.Session()
sess.run(tf.global_variables_initializer())

for step in range(2001):
    cost_val, hy_val, _ = sess.run([cost, hypothesis, train], feed_dict={x1: x1_data, x2: x2_data, x3: x3_data, Y: y_data})
    if step%10 == 0:
        print(step, "Cost: ", cost_val, "\nPrediction:\n", hy_val)
```

- 아래 코드는 x_data의 행렬을 갖고 가설을 만든 것이다.

```python
x_data = [[73., 80., 75.],
          [93., 88., 93.],
          [89., 91., 90.],
          [96., 98., 100.],
          [73., 66., 70.]]
y_data = [[152.],
          [185.],
          [180.],
          [196.],
          [142.]]

# placeholders for a tensor that will be always fed.
X = tf.placeholder(tf.float32, shape=[None, 3])
Y = tf.placeholder(tf.float32, shape=[None, 1])

W = tf.Variable(tf.random_normal([3, 1]), name='weight')
b = tf.Variable(tf.random_normal([1]), name='bias')

# Hypothesis
hypothesis = tf.matmul(X, W) + b

```


# 참고

[reduce_sum](http://blog.naver.com/PostView.nhn?blogId=wideeyed&logNo=221164644393)