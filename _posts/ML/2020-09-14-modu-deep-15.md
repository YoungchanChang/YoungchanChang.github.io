---
layout: post
title: 모두를 위한 딥러닝 15 - 학습 rate, overfitting, 그리고 일반화, train 실습
category: ML
tags: [ML]
comments: ML
---

### train

- 데이터 셋을 나눈다. 학습이 끝났을 때 사용할 데이터로 나눈다.

```python
x_data = [[1, 2, 1],
          [1, 3, 2],
          [1, 3, 4],
          [1, 5, 5],
          [1, 7, 5],
          [1, 2, 5],
          [1, 6, 6],
          [1, 7, 7]]
y_data = [[0, 0, 1],
          [0, 0, 1],
          [0, 0, 1],
          [0, 1, 0],
          [0, 1, 0],
          [0, 1, 0],
          [1, 0, 0],
          [1, 0, 0]]

# Evaluation our model using this test dataset
x_test = [[2, 1, 1],
          [3, 1, 2],
          [3, 3, 4]]
y_test = [[0, 0, 1],
          [0, 0, 1],
          [0, 0, 1]]

```

- 예측이 실제로 얼마나 맞는지 테스트하는 작업이다.

```python
# Correct prediction Test model
prediction = tf.argmax(hypothesis, 1)
is_correct = tf.equal(prediction, tf.argmax(Y, 1))
accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))
```


### Normalized inputs

- min_max_scalar()함수를 통해서 일반화를 시킬 수 있다. 데이터가 매우 클 때 사용하면 유용하다.

```python
xy = np.array([[828.659973, 833.450012, 908100, 828.349976, 831.659973],
               [823.02002, 828.070007, 1828100, 821.655029, 828.070007],
               [819.929993, 824.400024, 1438100, 818.97998, 824.159973],
               [816, 820.958984, 1008100, 815.48999, 819.23999],
               [819.359985, 823, 1188100, 818.469971, 818.97998],
               [819, 823, 1198100, 816, 820.450012],
               [811.700012, 815.25, 1098100, 809.780029, 813.669983],
               [809.51001, 816.659973, 1398100, 804.539978, 809.559998]])

# very important. It does not work without it.
xy = min_max_scaler(xy)
print(xy)

'''
[[0.99999999 0.99999999 0.         1.         1.        ]
 [0.70548491 0.70439552 1.         0.71881782 0.83755791]
 [0.54412549 0.50274824 0.57608696 0.606468   0.6606331 ]
 [0.33890353 0.31368023 0.10869565 0.45989134 0.43800918]
 [0.51436    0.42582389 0.30434783 0.58504805 0.42624401]
 [0.49556179 0.42582389 0.31521739 0.48131134 0.49276137]
 [0.11436064 0.         0.20652174 0.22007776 0.18597238]
 [0.         0.07747099 0.5326087  0.         0.        ]]
'''

x_data = xy[:, 0:-1]
y_data = xy[:, [-1]]
```


### 손글씩학습 실습

-  이미지를 byte로 나눈다. 28*28 = 784 pixel이 되게 된다. 이전에는 X의 input값이 0~10사이였다면 하나의 픽셀들을 전부 X의 input값으로 보게 되는 것이다.

- `from tensorflow.examples.tutorials.mnist import input_data` 텐서플로우에서 사용하는 라이브러리이다.

- epchos는 반복횟수, batch_size는 반복 때 한번에 얼마나 학습할 지 사이즈를 정해놓는 것이다.

```python
num_epochs = 15
batch_size = 100
num_iterations = int(mnist.train.num_examples / batch_size)
```

### 추가로 알아야 할 사항 -tf.argmax
- tf.argmax에 intput은 Tensor, rank이다. rank는 텐서의 원소 하나에 접근하기 위한 인ㄷ게스 개수이다. 1차원 배열의 경우는 최대 인덱스 개수가 1이기 때문에 0으로만 사용할 수 있다.

- tf.argmax(a, 0)는 2차원 배열의 각 열에서 가장 큰 값을 찾아 인덱스를 반환합니다.

- tf.argmax(a, 1)는 2차원 배열의 각 행에서 가장 큰 값을 찾아  인덱스를 반환합니다.. 

# 참조

[tf.argmax()](https://www.tensorflow.org/api_docs/python/tf/math/argmax)