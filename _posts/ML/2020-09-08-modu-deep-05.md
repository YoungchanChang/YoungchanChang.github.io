---
layout: post
title: 모두를 위한 딥러닝 05 - TensorFlow로 간단한 Linear regression을 구현
category: ML
tags: [ML]
comments: ML
---


# Linear Regression 구현

1. 텐서플로우 그래프 빌드

2. 세션.run()으로 실행을 시키고

3. 실행 결과를 돌린다.

### 텐서플로우 그래프 빌드하기

- Hypothesis H(x) = Wx + b

    - x와 y의 학습할 데이터가 주어진다. W와 b의 값들을 정의하는 것이 TF의 목적이다.

    - TF는 W와 b를 변수로 둔다. W와 b는 TF가 **학습시켜서 값을 얻는** 변수다. 자기가 변경시키는 변수이다.

    - W와 b의 값을 처음에 모르니깐 랜덤값을 준다.

    - 가설을 **W와 b의 텐서를 갖고 구성**을 할 수 있다. x_train * + W + b가 된다.

```python
import numpy as np
import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()

# X and Y data
x_train = [1, 2, 3]
y_train = [1, 2, 3]

W = tf.Variable(tf.random_normal([1]), name='weight')
b = tf.Variable(tf.random_normal([1]), name='bias')

# our hypothesis XW + b
hypothesis = x_train * W + b
```

- 가설 검증하기
    - 텐서가 변수로 둔 W와 b가 적절한 값인지 검증해야된다.
    - 손실함수인 cost = (1/m)*∑(H(xi) - yi)를 둔다.
    - reduce_mean은 값을 평균내주는 부분이다.

```python
cost = tf.reduce_mean(tf.square(hypothesis - y_train))
```

- 손실값을 최소화해야한다.
    - 텐서플로우는 값을 최소화하기위해 **옵티마이저를 정의**한다.
    - 무엇을 최소화할지 정해주면(cost) **텐서는 W와 b를 스스로 조정**한다.

```python
#Minimize
optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)
train = optimizer.minimize(cost)
```

### 세션을 만들기

- W와 b라는 tensorflow variable을 사용할기 전에는 반드시 초기화시켜줘야한다.

```python
#Launch the grpah in a session
sess = tf.Session()
# Initializes global variables in the graph
sess.run(tf.global_variables_initializer())
```

- W와 b의 최솟값을 구하는 노드를 실행시킨다.
    - minimize하는 node를 trian이라고 정의했다.
    - train을 실행을 시켜야 cost를 minimize한다.
    - W와 b를 최소화하게 된다.

```python
#Fit the line
for step in range(2001):
    sess.run(train)
    if step % 20 == 0:
        print(step, sess.run(cost), sess.run(W), sess.run(b))
```

### Placeholders

- 그래프를 빌드할 때 값을 바로 주지 않고 필요할 때 값을 던져준다.

- placeholder을 쓰는 이유는 나중에 **학습 데이터를 던져**줄 수 있기 때문이다.

- 가설을 placeholder로 세우기

```python

# 가설을 위한 노드를 다시 쓴다.
X = tf.placeholder(tf.float32, shape=[None])
Y = tf.placeholder(tf.float32, shape=[None])

hypothesis = X * W + b
```

- 기존에 train값만을 줬다면, placeholder에서는 **train과 함께 cost, W, b, 데이터**를 준다.

```python
    # Fit the line
    for step in range(2001):
        
        #sess.run(train)
        _, cost_val, W_val, b_val = sess.run(
            [train, cost, W, b], feed_dict={X: [1, 2, 3], Y: [1, 2, 3]}
        )
        if step % 20 == 0:
            print(step, cost_val, W_val, b_val)

```


### 트레인을 한다란?

- 그래프를 Build한다는 것은 `H(x) = Wx + b`노드로 된 hypotesis에 대해서 cost값을 구한 뒤 train의 결과값을 내서 W와 b를 조정하는 과정을 의미한다.

- 트레인을 실행시킨다는 의미는 그래프를 따라들어가서 W와 b를 조정하는 것을 의미한다.

<center>
<figure>
<img src="https://imgur.com/vlGBqIL.png" alt="views">
<figcaption>하나의 노드</figcaption>
</figure>
</center>

### 트레인 과정

1. 컴퓨터가 W와 b에 대해서 **가설**을 세운다. H(x) + b
2. 주어진 데이터를 넣었을 때와 실제의 **차이값**을 구한다. Cost Function
3. 차이값을 최소화하기 위해 **최적화(Optimizer)**를 한다.


# 전체 소스코드
```python
import numpy as np
import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()

# X and Y data
x_train = [1, 2, 3]
y_train = [1, 2, 3]

W = tf.Variable(tf.random_normal([1]), name='weight')
b = tf.Variable(tf.random_normal([1]), name='bias')

hypothesis = x_train * W + b

# cost / loss function
cost = tf.reduce_mean(tf.square(hypothesis - y_train))

#Minimize
optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)
train = optimizer.minimize(cost)

sess = tf.Session()
sess.run(tf.global_variables_initializer())

for step in range(2001):
    sess.run(train)
    if step % 20 == 0:
        print(step, sess.run(cost), sess.run(W), sess.run(b))
```


# 참조
[데이터 플로우 그래프](https://yamerong.tistory.com/40)

[텐서플로우 참고자료](https://github.com/hunkim/DeepLearningZeroToAll/tree/master/tf2)