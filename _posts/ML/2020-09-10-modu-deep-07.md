---
layout: post
title: 모두를 위한 딥러닝 07 - Linear Regression의 cost 최소화의 TensorFlow구현
category: ML
tags: [ML]
comments: ML
---

# cost 최소화 TensorFlow 구현

- 편의상 가설 함수를 H(x) = W * x로 둔다

- `cost = (1/m)*∑(H(xi) - yi)²`에서 W값이 바뀔 수록 cost값이 바뀐다.

- cost가 최저가 되는 W값을 찾아야 한다.

# 확인하기

- cost안에 hypothesis와 Y값이 들어가있고, hypothesis안에 X와 W가 들어가있다.

- X와 Y는 이미 주어진 데이터셋이며, W값을 (-30, 50)까지 조정함으로써 그래프의 변화를 직접 관찰한다.

- W가 1인 값이 최소화되게 된다.

```python
import numpy as np
import matplotlib.pyplot as plt
import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()

X = [1, 2, 3]
Y = [1, 2, 3]

W = tf.placeholder(tf.float32)

hypothesis = X * W

print("success")

# cost / loss function
cost = tf.reduce_mean(tf.square(hypothesis - Y))

# #Minimize
# optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)
# train = optimizer.minimize(cost)

sess = tf.Session()
sess.run(tf.global_variables_initializer())

# Weight값에 대응되는 cosv_val
W_val = []
cost_val = []

# cost안에 hypothesis와 Y값이 들어가있고, hypothesis안에 X와 W가 들어가있다.
# X와 Y는 이미 주어진 데이터셋이며, W값을 (-30, 50)까지 조정함으로써 그래프의 변화를 직접 관찰한다.
for i in range(-30, 50):
    feed_W = i * 0.1
    curr_cost, curr_W = sess.run([cost, W], feed_dict={W: feed_W})
    W_val.append(curr_W)
    cost_val.append(curr_cost)
    
plt.plot(W_val, cost_val)
plt.show()
```

### 수동으로 Gradient descent 학습시키기

- 기울기가 최고 낮은 점 보다 우측에 있으면 +가 되고, 좌측에 있으면 -가 된다. W는 -방향으로 움직여야 된다.

- 기울기가 최고 낮은 점 보다 좌측에 있으면 -가 되고, w는 +방향으로 움직여야 한다.

- `W := W - α * (1/m)*∑(W * (xi) - yi)(xi)`에서 기울기보다 낮으면 W는 +가되고, 높으면 -로 최적화되게 된다.

- 코드로 작성하면 아래와 같다.

 ```python
learning_rate = 0.1
gradient = tf.reduce_mean( (W * X - Y) * X)
descent = W - learning_rate * gradient

# 새로 나온 값을 할당함으로써 update시킨다.
update = W.assign(descent)

# 위의 코드를 간략히 하면 아래와 같다.
# optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)
# train = optimizer.minimize(cost)
 ```


# 참고

[reduce_sum](http://blog.naver.com/PostView.nhn?blogId=wideeyed&logNo=221164644393)