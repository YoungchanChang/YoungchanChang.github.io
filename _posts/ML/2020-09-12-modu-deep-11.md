---
layout: post
title: 모두를 위한 딥러닝 09 - Logistic(regression) Algorithm
category: ML
tags: [ML]
comments: ML
---

# Logistic Algorithm

- 분류 알고리즘중에 정확도가 높은 알고리즘

- Neural Network과 Deep Learning에 중요한 알고리즘

### Regression (HCG)

- hypothesis, Cost, graident descent로 진행된다.

- Cost는 학습 데이터와 가설로 세운 선과의 차이를 의미한다.

- 학습이란 최소화하는 Weight를 찾아내는 것이다.

- 산을 타고 내려가는 것 처럼 기울기를 본다. 그 다음에 한발짝씩 움직인다.

- 기울기는 Cost함수를 미분한 값이다. 한 번에 얼마나 움직일까가 알파값이다. stepsize. learning rate라고 부른다.

### Classfication

- 숫자를 예측하는 것이 아니라, Binary 둘 중의 하나 정해진 카테고리를 고르거나,

- 예시) 이메일을 보냈을 때 스팸이 될지, 햄이 될지 / facebook에서 신나고 재밌는 timeline보기, 무수히 많은 timeline을 facebook은 보여주지 않는다. facebook에서 classficiation알고리즘을 이용해서 내가 좋아요 선택한 기사들을 갖고 학습을 해서 친구들이 만들어내는 수백개의 타임라인 중에 어떤 라인을 보여주고 어떤 라인을 보여주지 말자 결정한다. 굉장히 좋은 것들만 보여준다. facebook에는 좋아하는 timeline만 있다. 기존의 학습과 다른 패턴이면 가짜라고 예측할 수 있다.

- 스팸 감지 / 페이스북 피드 / 신용카드 사기 거래

- Radiology 이미지를 보고 나쁜 tumor인지 괜찮은 tumor인지 확인한다. 주식을 보고 살까 팔까결정한다.

### Pass(1)/Fail(0) based on study hours

- 몇 시간을 공부했더니 통과하냐 떨어지냐

- Linear regression을 두고 할 수 있을 것이다.

- 0.5되는 점을 찾아서 더 큰 값을 예측하게 되면 통과, 작은 값이면 불합격으로 예측할 수 있다.

- W*x + b로 했을 때 문제점? 선을 보고 0.5만으로 예측하면 기울어지다보니깐 값이 변한다.

- 또 다른 문제점 : 반드시 값이 0과 1의 사이 값을 갖어야 한다.

- 0보다 훨씬 크거나 작거나 1보다 큰 값이 나오게 된다.

- x의 값이 1,2,5,10시간이라면 0에서 1사이의 값들을 학습을 시키고 예측을 하려고 한다면,

- 어떤 학생이 100시간을 했을 때, 그대로 모델에 적용한다면 0.5로 적용한다면

- 50이 예측이 된다. 1보다 무지 큰 수가 된다.

- Linear Regression을 0과 1사이로 압축을 시켜주는 형태의 함수!!!

- g(z)는 z의 상관없이 0에서 1사이를 만들어주면 좋겠다. `g(z) = 1/(1 + e^(-z))`

- 시그모이드 S가 2개가 걸쳐있다.

- z가 커지면 g함수의 값은 1에 가까워진다. z가 작아지면 함수의 값은 0에 가까이 가기만 할 뿐 0보다 큰 값을 갖게 된다.

- z를 wx로 두고 새로운 hypotesis를 H(x) = g(z)로 두면 된다.

- 논리 가설에서는 `H(x) = 1/(1 + e^(-WTX)`)로 쓰면 된다.

- Logistic Classification가설의 함수가 되도록 쓴다. cost를 구하고 cost를 minimize한다.

### cost Functino과 최소화하는 알고리즘

- 시그모이드 함수를 이용해서 0 < ~ < 1 값을 구한다. `H(x) = 1/(1 + e^(-WTX)`)

- Linear Regression의 가설인 `H(x) = W * x  + b`의 손실함수로 `cost = (1/m)*∑(H(xi) - yi)²`를 썼다.

- 시그모이드 함수를 손실함수로 이용하면 울륵불륵한 이상한 2차함수가 나오게 된다.

- 시그모이드는 더 이상 Linear한 term이 아니게 된다. Shape이 구불한 커브 모양이 나오게 된다.

- 구불구불하게 함수가 나오게 되면, 경사타고 내려가기 알고리즘을 적용했을 때 시작점에 따라 끝나는 점이 달라진다.

- 평평해지고 기울기가 없어졌을 때, 최저점을 예측했다면 최저점 예측이 잘 안된다. 어느 지점에서 시작했느냐에 따라서 달라지게 된다.

- Local Minimum 최저점이 된다. 최종적인 전체에서 minimum은 global minimum을 찾는게 목표가 된다. cost함수가 생겨먹으면 멈춰버리게 된다. cost의 학습이 멈추면 model이 나쁘게 prediction된다. 

- 변하는 cost함수 : cost(W) = 1/m ∑ c(H(x), y) 하나의 손실 함수는 아래와 같다.

- `c(H(x), y) {-log(H(x)) : y = 1 , -log(1 - H(x)) : y = 0}` c함수는 c(H(x), y)값을 갖고 정의한다. y가 1일때나 0일때로 정의한다.

- Hypothesis가 1+e^-2일때 expression이 구부러진 것을 잡는 것이 log함수이다. log함수를 사용하는 것이 기본적인 idea이다. 실제로 log함수의 모양을 보면 잘 맞게 된다. g(z) = -log(z)라고 봤을 때, z가 1보다 큰 값은 나올 수 없다. z가 0이되면 함수의 값은 굉장히 커지게 된다.

- cost function? 실제 값과 예측한 값이 같으면 cost값은 작아지고, 예측한 값이 크게 틀리면 cost값이 커지게 해서 Model에게 벌을 주게 된다. cost를 최소화하기 위해서 weight를 병행시키게 된다. y=1이라고 할 때, H(x)의 예측값이 1로 맞았다면, cost는 함수가 1일때 0에 가까워지게 된다. label이 1이고 예측이 1이면 cost가 0이 된다.

- H(x)가 0이 되서 틀리다면 cost값은 매우 커진다. 무한대로 가깝게 가게 된다. 예측을 틀리면 cost가 높아져서 시스템에 벌을 준다.











