---
layout: post
title: 모두를 위한 딥러닝 14 - 학습 rate, overfitting, 그리고 일반화, train
category: ML
tags: [ML]
comments: ML
---

# 실제 사용할 때 TIP

### Learning rate

- Learning rate(α)이 크다면 overshooting문제 발생

<center>
<figure>
<img src="https://imgur.com/0x0QBGV.png" alt="views">
<figcaption>overshooting문제</figcaption>
</figure>
</center>

- 작다면 너무 오래걸리거나 지역에서 멈춘다.

### Data Proprocessing

- 데이터 값의 큰 차이가 있을 때 **normalize할** 필요가 있따.

<center>
<figure>
<img src="https://imgur.com/AeJHbHV.png" alt="views">
<figcaption>다른 곳으로 튈 수 있다.</figcaption>
</figure>
</center>

### Overfitting

- 특정한 데이터에 맞게 구부리는 것을 의미한다.

- 해결책은 **weight이 적은 값**을 갖게 된다. 구부리지 말고 펴! weight을 작게줘

- cost를 최적화, 최소화시켜. 

- W의 각각의 값에 제곱이 작아질 수 있도록 쫙쫙펴줘

<center>
<figure>
<img src="https://imgur.com/gEPmtA1.png" alt="views">
<figcaption>다른 곳으로 튈 수 있다.</figcaption>
</figure>
</center>

### Train/Testing dataset

- 기존의 데이터로만 학습을 하면 데이터 자체를 **암기**할 수 있다.

- 테스트용 데이터를 따로 두면 된다. 시험 때 모의고사를 보고 수능을 보는 것과 같다.

- α(learning rate), Regulization(람다) 를 튜닝할 때 Training, Validation, Testing으로 나눈다.

- validation의 어떤 것이 좋을까 튜닝(실전 모의고사)을 한 뒤에 테스트(모의고사)를 본다.

### Online learning

- 데이터 셋이 많을 때 한 번에 학습시키기 어렵다.

- 100만개가 있을 때 10만개씩 나눌 수 있다. 이전에 있는 데이터에 추가로 학습을 시킬 수 있다.

