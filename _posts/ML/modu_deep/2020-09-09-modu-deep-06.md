---
layout: post
title: 모두를 위한 딥러닝 06 - Linear Regerssion의 cost 최소화 알고리즘의 원리 설명
category: ML
tags: [ML]
comments: ML
---

# 손실함수 최소화하는 알고리즘의 원리

- 가설은 H(x) = W * x + b이다. 실제 모델과 얼마나 다른지 Cost Function을 정의해준다.

- 손실함수는 `cost = (1/m)*∑(H(xi) - yi)²` cost를 최소화시키는 W와 b를 찾는 것이다.

### 손실 함수의 구체적인 예시

1. x = 1 y = 1 , x = 2 y = 2 , x = 3 y = 3의 데이터가 주어졌고, 컴퓨터는 `H(x) = Wx + b`가설을 세운다.

2. 컴퓨터는 먼저 W = 1이라고 가정을 하고, 주어진 x와 y의 데이터를 넣어서 **cost(W)**의 값을 구한다.

3. cost(W)의 값은 ( 1 x 1 - 1)² + (1 x 2 - 2)² + (1 x 3 - 3)² = 0으로 된다.

4. W = 0일 때, 같은 방법으로 하면 cost(W) = 4.67이 된다.

5. W = 2일 때, 같은 방법으로 하면 cost(W) = 4.67이 된다.

6. W에 대한 함수를 그리면 손실함수 `cost = (1/m)*∑(H(xi) - yi)²`는 2차함수의 값을 갖고 컴퓨터는 2차함수의 최솟값을 찾아야 한다.

<center>
<figure>
<img src="https://imgur.com/4uJsICJ.png" alt="views">
<figcaption>2차 함수</figcaption>
</figure>
</center>

### Gradient descent Algorithm, 경사하강법

- 주어진 2차함수의 cost function을 minimize하는데 사용되는 방법이다.

- 경사도는 **미분**을 통해 구하고, 특정 지점에서 경사도가 낮은 지점을 찾아야 한다.

- W와 b 조금 바꾸면 cost를 줄이게 된다. 그 다음에 경사도를 계산해서 계속해서 과정을 반복한다.

### 경사도 구하기 - Formal definition

- 손실함수를 `cost = (1/m)*∑(H(xi) - yi)²`를 2로 나누면 우측의 식이 된다. `cost = (1/2m)*∑(H(xi) - yi)²`

- 우측의 식을 미분하면 공식적인 알고리즘 - `W : = W - α * ( δ / δ * W ) * cost(W)`

- W를 현재 있는 W의 값에서 알파값(Leartning rate)에 cost함수를 미분한 것을 뺀다. 미분은 그 점의 기울기를 구한다.

- 미분해서 기울기가 양수라면 공식적인 알고리즘의 값이 줄어든다. 반면에 음수라면 공식적인 알고리즘의 값이 늘어난다.

- W를 조금 더 큰 값으로 움직이겠다는 표현이다.

- 제곱에 미분을 하게 된다면, `W := W - α * (1/m)*∑(W * (xi) - yi)(xi)`이런 식이 나온다.

- 여러번 실행시키게 되면 W값이 계속 변하게 된다. 그러면 cost를 minimize하는 값이 자동으로 구해진다.


### Convex function

- W와 b의 값에 대해 cost값의 관계는 변수가 3개임으로 3차원으로 그릴 수 있다.

- 처음 주어진 값에 따라서 값의 최솟값이 달라질 수 있다.

<center>
<figure>
<img src="https://imgur.com/IYYOrLb.png" alt="views">
<figcaption>3차 함수</figcaption>
</figure>
</center>

- Cost Function을 설계할 때, Convex function이 된다면 아래와 같은 그림이 나오게 된다.

<center>
<figure>
<img src="https://imgur.com/6WgpOP0.png" alt="views">
<figcaption>Convex function</figcaption>
</figure>
</center>

- 따라서, 어느 지점에서 시행하든 도착하는 지점이 원하는 지점이 된다. 항상 답을 찾는 것을 보장해준다.

