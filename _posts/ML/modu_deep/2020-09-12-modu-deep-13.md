---
layout: post
title: 모두를 위한 딥러닝 13 - Softmax Regression 실습
category: ML
tags: [ML]
comments: ML
---

# Linear Regression과 SosftMax Regression과 Logistic의 차이점

- 지도학습은 가설(hypothesis) => 결과(Cost) => 재조정(Gradient)로 이루어진다.

- **회귀 모델(regression)** : 두 변수의 선형 **상관관계를 모델링** 하는 방식

    - 회귀모델의 가설값으로 H(x) = W * x + b가 설정된다. `hypothesis = tf.matmul(X, W) + b (multi일 때)`
    
```python
# Hypothesis
hypothesis = x_train * W + b
hypothesis = tf.matmul(X, W) + b (multi일 때)

# Simplified cost/loss function
cost = tf.reduce_mean(tf.square(hypothesis - Y))

# Minimize
optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-5)
train = optimizer.minimize(cost)
```

- **이진 분류 모델(binary classfication)** : 상호 배타적인 두 클래스 중 하나를 출력하는 분류 작업 유형

    - 이진 분류 모델의 가설은 (x) = W * x + b를  sigmoid한 값이다. `hypothesis = tf.sigmoid(tf.matmul(X, W) + b)`

```python
# Hypothesis using sigmoid: tf.div(1., 1. + tf.exp(tf.matmul(X, W)))
hypothesis = tf.sigmoid(tf.matmul(X, W) + b)

# cost/loss function
cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) *
                       tf.log(1 - hypothesis))

train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)
```

- **다중 레이블 분류(multi-label classification)** : 셋 이상의 클래스 사이에서 구분하는 분류 문제

```python
# tf.nn.softmax computes softmax activations
# softmax = exp(logits) / reduce_sum(exp(logits), dim)
hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)

# Cross entropy cost/loss
cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))

optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)
```


### 주의할 점

- Weight를 쓸 때 주의할 점 [행 x 열] x [열 x 결과값]이 된다. 열의 개수를 인자로 줘야한다는 것을 잊지 말자.


- Softmax Regression은 X값 으로 Y의 값을 추론한 다음에, 각각에 sigmoid함수를 쓰지 않고 합들에 대해서 softmax로 처리한다. **시그모이드가 아닌 softmax를 쓰면 확률**이 된다. 값이 여러개임으로, w의 값도 여러개가 된다.

- 채점을 할 때는 원핫인코딩을 쓴다. 예측 값은 확률이 되게 된다. SOFTMAX 또는 확률이라고 이야기한다.

- 기존 식과 달라진 점 : 1. 결과값을 원핫인코딩한다. 원핫인코딩은 cost/loss에 쓰인다. 2.가설에 softmax함수를 쓴다.

```python
Y_one_hot = tf.one_hot(Y, nb_classes)  # one hot
print("one_hot:", Y_one_hot)
Y_one_hot = tf.reshape(Y_one_hot, [-1, nb_classes])
print("reshape one_hot:", Y_one_hot)
```

- X의 값은 Y일 것이다. X의 값은 Y의 점수가 나왔고 이것은 참일 것이다. X의 값들을 모아보니 Y의 클래스들이 나왔고 이것은 참일 것이다.