---
layout: post
title: 모두를 위한 딥러닝 23 - Weight 초기화 잘해보자
category: ML
tags: [ML]
comments: ML
---

# 초기값 주기

- weight을 입력할 때 random값을 줬고, random값에 따라서 결과가 달라진다.

- 처음 Weight값을 0으로 준다면, W*x + b로 곱해지기 때문에 학습이 일어나지 않게 된다.

- weight 잘 주기 : 절대로 모든 weight에 0을 주면 안 된다.

# 초기화시키는 방법

- Restricted Boatman Machine(RBM)으로 해결했다.

- 입력값이 있을 때, Backward를 바로 실행함으로써 입력을 재생성하는 것이 목적이다.

- Weight은 그대로 사용하는데, 받아온 값을 이용해서 거꾸로 쏴준다.

- 첫번째 줬던 값과 거꾸로 쏜 생성된 x의 값을 비교한다. 해당 값이 최저가 되도록 weight를 조절한다.

- Forward로 가는 것을 encode라고 하고, Backward를 decode라고 부르기도 한다.

<center>
<figure>
<img src="https://imgur.com/IfdUeHg.png" alt="views">
<figcaption></figcaption>
</figure>
</center>

# RBM을 통해 초기화시키는 방법

- 처음에 2개의 값에 대해서만 준 값이 유사하게 되도록 줬다뺏다 한다. 그 다음, 그 다음으로 가서 학습을 시킨다.

- Deep Belief Net이라고 한다.

- X라는 값에서 2개를 갖고 Restricted Boatman Machine을 돌린다. 그 다음 Layer에서도 RBM을 돌린다.

- Network전체를 보면 Weight들이 전부 초기화된 값이 된다.

<center>
<figure>
<img src="https://imgur.com/k7Cal07.png" alt="views">
<figcaption></figcaption>
</figure>
</center>


# 다른 방법들

- Xavier initalization 방법도 있다. 입력값과 출력값에 비례해서 초기값을 준다.

- He initialization

- weight의 값을 fan_in과 fan_out으로 나눠서 주면 된다.

<center>
<figure>
<img src="https://imgur.com/hdt7jEv.png" alt="views">
<figcaption></figcaption>
</figure>
</center>
