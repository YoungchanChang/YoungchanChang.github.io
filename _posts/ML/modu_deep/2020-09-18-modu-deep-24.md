---
layout: post
title: 모두를 위한 딥러닝 24 - Dropout과 앙상블
category: ML
tags: [ML]
comments: ML
---

# Overfitting

- 학습을 너무 많이하면 학습 곡선이 꾸불꾸불해진다. 마치 공부를 했을 때 암기만 하는 것과 같다. 마치 실전에서는 문제를 못 푸는 것과 같다.

- Layer가 많아질 수록 에러는 떨어지는데, 실전 데이터를 사용하면 어느 순간부터 에러가 올라가게 된다.

- 모델은 학습됬는데 과정이 안 좋다. overfitting이라고 한다.

<center>
<figure>
<img src="https://imgur.com/i9pK7y5.png" alt="views">
<figcaption></figcaption>
</figure>
</center>

- 깊게 만들수록 overfitting될 수 있다. 많은 변수들이 사용되고,  선이 꼬불꼬불하게 된다.

<center>
<figure>
<img src="https://imgur.com/rqiirSv.png" alt="views">
<figcaption></figcaption>
</figure>
</center>


# Solutions for overfitting

- 학습데이터가 많거나 Regularization을 쓰면 된다. 

<center>
<figure>
<img src="https://imgur.com/4uU1ZqQ.png" alt="views">
<figcaption></figcaption>
</figure>
</center>

# Dropout

- 그만 둬버리기

- Neural Network를 배운 것을 인위적으로 끊어서 몇 개의 노드를 죽인다.

- 랜덤하게 뉴런들을 죽여버린다.

<center>
<figure>
<img src="https://imgur.com/NpbFujG.png" alt="views">
<figcaption></figcaption>
</figure>
</center>

# 왜 드랍아웃이 됄까?

- 각 뉴런들이 전문가라고 했을 때 훈련할 때 몇멸은 쉬게 하고 나머지만 갖고 훈련시킨다. 랜덤하게 쉬게해서 훈련을 시킨다.

- 쉬게해놓고 나머지 애들만 갖고 맞추게 한다. 마지막에 총 동원해서 예측을 한다. 더 잘될 수 있다.

- 주의할 점은 학습할 때만 드랍아웃시킨다. 실전에는 전문가 모두를 불러와야한다. 실전에는 반드시 dropout_rate를 1로 준다.

<center>
<figure>
<img src="https://imgur.com/cPTzeO9.png" alt="views">
<figcaption></figcaption>
</figure>
</center>


# Ensemble

- 기계가 많고 학습시킬 것이 많을 때 사용된다.

- 독립적으로 Neural Network를 만든다. 똑같은 형태의 Neural Network를 만든 다음에 학습을 시킨다. 초기값이 다르니깐 결과가 다르게 된다. 마지막에 합친다.

- 마치 전문가 10명에게 물어보는 것과 같다. 최소 2%에서 4~5%는 성능 향상이 된다.

<center>
<figure>
<img src="https://imgur.com/JO30cCT.png" alt="views">
<figcaption></figcaption>
</figure>
</center>

# 추가로 드는 나의 생각

- overfitting처럼 하나의 정답만을 찾지 않게 하려는 것이 드랍아웃 아닐까? 어느 상황에서 정답은 A와 A`이 될 수도 있는데 A만을 고집하는 것을 막는 것이 아닐까?