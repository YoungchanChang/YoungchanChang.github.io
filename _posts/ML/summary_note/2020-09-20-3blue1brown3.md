---
layout: post
title: 강의정리 04 - 3Blue1Brown 02- Learning
category: ML_Summary
tags: [ML]
comments: ML
---

# 역전파

- 복잡한 기울기를 계산하기 위한 알고리즘

- weight값들의 벡터인 1만 3천개를 **13000차원의 방향**으로 생각하는 것은 상상의 범위를 넘어선다.

- 각 오차는 오차함수가 각 가중치 및 편차에 얼마나 민감한지를 나타낸다.

- 각 weight의 기울기는 달라진다. **오차함수의 기울기가 0.1인 가중치보다 기울기가 3.2인 가중치가 32배 더 민감**하다고 해석할 수 있다.

<center>
<figure>
<img src="https://imgur.com/hSnIKy7.png" alt="views">
<figcaption></figcaption>
</figure>
</center>


- 수많은 작은 조정들이 겹쳐잇을 뿐이다.

# 직관적으로 알기

- 경사하강법에서 가중치와 편차를 조정하는 것은 모든 예제마다 다르다. 그래서 2의 이미지 만을 예씨로 했을 때 가중치와 편향을 어떻게 조정할 수 있을까

<center>
<figure>
<img src="https://imgur.com/GkNHFWY.png" alt="views">
<figcaption></figcaption>
</figure>
</center>


- 결과값은 랜덤하다. **가중치와 편향에 변화**을 주면 된다.

- 출력층의 어떤 값이 조정되는지를 알아야 한다. 조정의 크기는 현재 값이 목표 값에서 얼마나 떨어져있는지 계산해야된다.

- 어떻게 increase되냐. 이전 계층의 모든 활성화에 대한 특정 가중치 합계와 편향으로 계싼한다.

- bias를 높이거나, weight를 증가시키거나, 인풋값을 변화시키거나.

- **직접 연관된 뉴런들**은 오차함수에 실제로 더 강한 영향을 미친다.

<center>
<figure>
<img src="https://imgur.com/FmKbZiL.png" alt="views">
<figcaption></figcaption>
</figure>
</center>


- 오차함수의 값을 줄여야 한다. 가중치 증가, 연결 강화가 이루어져야 한다. **가장 활동적인 뉴런에서 연결**된다.



- 이전계층의 모든 활성화를 변경한다. 양의 뉴런을 활성화시키면 값이 증가하고, (뉴런이 더 활발하게 움직이게 되고), 음의 뉴런을 활성화시키면 값이 감소한다.

- weight을 늘리는 것은 a에 비례하고, a를 늘리는 것은 weight에 비례한다. (값을 미분해보면 a와 b가 서로에게 비례한다는 것을 알 수 있다.)

- 각각의 뉴런들도 마지막층에 대한 생각을 갖고 있게 된다.

<center>
<figure>
<img src="https://imgur.com/ypv0ExU.png" alt="views">
<figcaption></figcaption>
</figure>
</center>

# 역전파 아이디어

- 이러한 모든 효과를 합치면, 두번째층에서 마지막층에 어떻게 변화해야하는지 알 수 있다.

- 똑같은 방식을 이전 층에도 적용할 수 있다.

<center>
<figure>
<img src="https://imgur.com/ypv0ExU.png" alt="views">
<figcaption></figcaption>
</figure>
</center>

- 모든 트레이닝에 대해 동일한 루틴을 수행하면 된다.

- 역전파는 하나의 훈련이 가중치와 편향을 조금씩 움직이는 것을 결정하기 위해 변화에 대한 상대적인 비용을 빠르게 감소시키는 것을 보는 것이다.


# 미적분풀이

- 머신러닝을 할 때 네트워크의 관점에서 연쇄법칙을 생각하면 된다.

<center>
<figure>
<img src="https://imgur.com/FoHJENo.png" alt="views">
<figcaption></figcaption>
</figure>
</center>

- 우리의 목표는 비용함수(cost function)이 변수에 대해서 얼마나 민감하게 변하는지 이해하는 것이다.

<center>
<figure>
<img src="https://imgur.com/43zcK4B.png" alt="views">
<figcaption></figcaption>
</figure>
</center>

- 가중치와 편향을 바꾸는 것이 비용 함수를 효율적으로 낮추는지 알 수 있다.

- 활성화정도는 가중치와 뉴런의 활성화정도와 편향을 더한 것이다.  시그모이드 렐루함수에 쓴 것이다.

<center>
<figure>
<img src="https://imgur.com/QUHAU0x.png" alt="views">
<figcaption></figcaption>
</figure>
</center>

- 가중치가 결과에 얼마나 영향을 미치는지 알아야 된다. weight의 변화 1개가 비용에 얼마나 바뀌는지 알아야 한다. 비율을 알아야 한다!

<center>
<figure>
<img src="https://imgur.com/aoSykIc.png" alt="views">
<figcaption></figcaption>
</figure>
</center>

- w(L)의 변화는 z(L)에 영향을 미치고, z(L)은 a(L)에 영향을 미치게 된다.

<center>
<figure>
<img src="https://imgur.com/UV2OMHt.png" alt="views">
<figcaption></figcaption>
</figure>
</center>

- 그러기 위해서는 먼저 z가 w에 미치는 영햐응ㄹ 알아야 한다. z(L)의 변화와 a(L)의 변화를 알아야 한다. 3개의 비율을 곱하는 것으로 민감도를 알 수 있다.

# 미적분 시작

- 이 크기가 네트워크 출력과 비례하게 된다. 출력이 다르면 약간의 변화도 큰 영향을 미치게 된다.

<center>
<figure>
<img src="https://imgur.com/wCHzeHd.png" alt="views">
<figcaption></figcaption>
</figure>
</center>

- 가중치에 생긴 변화가 마지막층에 미치는 영향은 그 전의 뉴런이 얼마나 강한지에 달려있게 된다. 

<center>
<figure>
<img src="https://imgur.com/QiDjA1U.png" alt="views">
<figcaption></figcaption>
</figure>
</center>