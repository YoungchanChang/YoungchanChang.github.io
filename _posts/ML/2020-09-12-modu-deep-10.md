---
layout: post
title: 모두를 위한 딥러닝 08-02 - Linear Regression정리하기
category: ML
tags: [ML]
comments: ML
---

# Hypothesis 가설세우기

- 데이터 [1, 2, 3]과 [4, 5, 6]이 주어졌을 때 컴퓨터의 목적은 두 변수의 선형 **상관관계를 모델링** 하는 것이 목적이다.

- 컴퓨터는 주어진 데이터에 `H(x) = W * x  + b`라는 가설을 세운뒤에 W와 b의 값을 추론한다.

- 컴퓨터가 실제 데이터 [1, 2, 3]과 [4, 5, 6]을 가설에 대입했을 때 어떤 값이 나온다.

# Cost Function

- 컴퓨터는 컴퓨터가 세웠던 `H(x) = W * x  + b`라는 **가설이 얼마나 맞는지 확인**한다.

- 손실함수로 `cost = (1/m)*∑(H(xi) - yi)²` 모든 값을 더한 뒤에 평균을 낸다. 해당 식은 이차함수의 형태로 나오게 된다.

- 컴퓨터는 해당 이차함수를 통해 W와 b의 값과 목표로 여기는 값의 차이를 알게 된다.

# Gradient descent Algorithm

- 컴퓨터는 이차함수의 **기울기를 이용**해 경사각도가 완만한 지점으로 가게 된다.

- 기울기를 구하기 위해 **미분을 이용**하고, w값을 어느정도 이동할 것인지 α(learning rate)을 설정한다.

- 미분된 식을 통해서 `W := W - α * (1/m)*∑(W * (xi) - yi)(xi)` 경사하강을 재조정하면 된다.

- 여기서 `yi`의 값은 데이터의 실제 값이고 W값을 구해야 한다. α값은 내가 임의로 준 지점이다.

