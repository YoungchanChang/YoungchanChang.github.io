---
layout: post
title: 모두를 위한 딥러닝 08-02 - Linear Regression정리하기
category: ML
tags: [ML]
comments: ML
---

# Hypothesis 가설세우기

- 데이터 [1, 2, 3]과 [4, 5, 6]이 주어졌을 때 컴퓨터의 목적은 두 변수의 선형 **상관관계를 모델링** 하는 것이 목적이다.

- 컴퓨터는 주어진 데이터에 `H(x) = W * x  + b`라는 가설을 세운뒤에 W와 b의 값을 추론한다.

- 컴퓨터가 실제 데이터 [1, 2, 3]과 [4, 5, 6]을 가설에 대입했을 때 어떤 값이 나온다.

# Cost Function

- 컴퓨터는 컴퓨터가 세웠던 `H(x) = W * x  + b`라는 **가설이 얼마나 맞는지 확인**한다.

- 손실함수로 `cost = (1/m)*∑(H(xi) - yi)²` 모든 값을 더한 뒤에 평균을 낸다. 해당 식은 이차함수의 형태로 나오게 된다.

- 컴퓨터는 해당 이차함수를 통해 W와 b의 값과 목표로 여기는 값의 차이를 알게 된다.

# Gradient descent Algorithm

- 컴퓨터는 이차함수의 **기울기를 이용**해 경사각도가 완만한 지점으로 가게 된다.

- 기울기를 구하기 위해 **미분을 이용**하고, w값을 어느정도 이동할 것인지 α(learning rate)을 설정한다.

- 미분된 식을 통해서 `W := W - α * (1/m)*∑(W * (xi) - yi)(xi)` 경사하강을 재조정하면 된다.

- 여기서 `yi`의 값은 데이터의 실제 값이고 W값을 구해야 한다. α값은 내가 임의로 준 지점이다.

- 미분한 식은 오른쪽과 같다. `W : = W - α * ( δ / δ * W ) * cost(W)`

### Regression (HCG)

- 인공지능의 학습은 hypothesis, Cost, graident descent로 진행된다.

- 여기서 Cost는 **학습 데이터와 가설**로 세운 선과의 **차이 값**를 의미한다.

- 학습이란 차이값을 **최소화하는 Weight과 bias**를 찾아내는 것이다.

- 차이값이 2차함수로 나왔을 때, 산을 타고 내려가는 것 처럼 **기울기**를 본다. 그 다음에 한발짝씩 움직인다. **한발짝이 α로 주어진 learning rate**이다.

- 기울기는 **Cost함수를 미분**한 값이다. 한 번에 얼마나 움직일까가 알파값이다. stepsize. learning rate라고 부른다.
