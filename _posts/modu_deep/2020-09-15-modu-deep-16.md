---
layout: post
title: 모두를 위한 딥러닝 16 - 딥러닝의 기본 개념, 시작과 XOR문제
category: ML
tags: [ML]
comments: ML
---

# 딥러닝의 기본 개념

- 생각하는 기계를 만들자! 뇌를 본따서 만들자!

- 뇌의 특징 1.뇌가 복잡히 연결 2. 부분부분들의 neuron unit이 단순히 동작이 된다.

- input에 전해주는 길이에 따라 신호의 양이 달라진다. x*weight(신호의 양)에 sum이 일어난다.

- 통과가되면서 bias가 추가되면서 전달된다. 다 모여져있는 값이 어떤 값 이상이 되면 활성화되고, 이상이 되지 않으면 활성화가 되지 않는다.

### Activation Function

- 기계적, 수학적으로 만들어낸 모델이 Activation Functions

- 값을 모으는 것이 있을 때, 어떤 값으로 곱하게(Weight) 된다. bias로 합하게 되면 activation function에서 일정한 값이 넘으면 1이라는 신호를 준다. 값이 넘지 않으면 0의 신호를 준다.

- Logistic regression units를 모은다면 **동시에 출력**할 수 있게 된다.

### XOR문제 풀기

- AND와 OR문제. x1과 x2를 입력했을 때 **OR나 AND에 맞는 선**을 만들어낼 수 있을까?

- XOR문제는 선형으로 만들 수 없다. 어떻게 선을 그어도 안 된다.

- **MLP(multilayer perceptrons)로 XOR문제**를 풀 수 있지만, w와 b를 **학습**을 시킬 수가 없다. 재조정이 불가능하다

### Backpropagation

- w와 b가 틀렸을 때 w와 b를 조절을 해야한다! **뒤로 전달을 하며 각각을 전달하는 알고리즘**이 있다.

### Convolutional Neural Networks

- 고양이를 갖고 그림을 보게 한 다음에 시신경의 뉴런 동작을 보니깐 그림의 형태에 따라 일부의 뉴런들만 활성화되고 다른 그림을 주면 다른 뉴런이 활성화된다. 신경망세포가 그림 전체를 보는 것이 아니라, 일부의 부분부분들을 담당하는 신경망들이 있고 조합되는 것이 아닐까?

- 한 번에 입력하는 것이 아니라 일부분을 잘라서 레이어로 보낸다. 부분부분으로 자르고 나중에 합치는 방법을 쓴다. 알파고도 이 방법을 쓴다.

### A Big Problem

- Backprogation은 몇 개의 Layer에서는 잘 동작하지만, 복잡한 문제를 풀려면 10여개 이상의 Layer를 학습해야한다. BackPropagtion은 에러를 뒤로 갈 때 feedback을 주기 힘들게 된다. 많이 할 수록 성능이 떨어지게 된다.

- SVM, RandomForest알고리즘도 있다.


### 딥러닝의 출현

- 06, 07년에 두 논문을 발표한다.

<center>
<figure>
<img src="https://imgur.com/r5LbBDn.png" alt="views">
<figcaption></figcaption>
</figure>
</center>

- 06년 딥한 신경망을 학습을 하기 위한 조건 : 초기값을 잘못줬다. 초기값을 잘 선택한다면 학습할 수 있다.

- 07년 논문 : 깊게 신경망을 구축하면 복잡한 문제를 풀 수 있다.

### 딥러닝

- 이미지Net Challenge에서 딥러닝이 우수한 효과를 발휘했다.

- 그림을 설명할 수도 있다.  